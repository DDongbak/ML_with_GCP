{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# intermax os, stat, wait table을 전처리한 데이터에 대해 **RNN** 모델을 만들자\n",
    "\n",
    "by pdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----------------------------------------\n",
    "# Library used for command parsing (maybe)\n",
    "# ----------------------------------------\n",
    "import argparse\n",
    "import os\n",
    "from tensorflow.contrib.learn.python.learn.utils import (\n",
    "    saved_model_export_utils)\n",
    "from tensorflow.contrib.training.python.training import hparam\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Library used for loading a file from Google Storage\n",
    "# ---------------------------------------------------\n",
    "from tensorflow.python.lib.io import file_io\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# Library used for uploading a file to Google Storage\n",
    "# ---------------------------------------------------\n",
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def MinMaxScaler(data):\n",
    "    ''' Min Max Normalization\n",
    "    Parameters, Returns\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        input data to be normalized\n",
    "        shape: [Batch size, dimension]\n",
    "    '''\n",
    "    numerator = data - np.min(data, 0)\n",
    "    denominator = np.max(data, 0) - np.min(data, 0)\n",
    "    \n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_series(filename):\n",
    "  filename = filename[0] #filename : route (--train-files <route>)\n",
    "  try:\n",
    "    with file_io.FileIO(filename, mode='r') as csvfile:\n",
    "      csvreader = csv.reader(csvfile)\n",
    "      return csvreader\n",
    "  except IOError:\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_experiements(hparams):\n",
    "  \n",
    "  data = load_series(hparams.train_files)\n",
    "  \n",
    "  # train Parameters\n",
    "  seq_length = 10\n",
    "  data_dim = 53\n",
    "  hidden_dim = 10 #??\n",
    "  output_dim = 1\n",
    "  learning_rate = 0.01\n",
    "  iterations = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ml-engine돌릴때 argument parsing을 위해 필요한 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "          <script src=\"/static/components/requirejs/require.js\"></script>\n",
       "          <script>\n",
       "            requirejs.config({\n",
       "              paths: {\n",
       "                base: '/static/base',\n",
       "              },\n",
       "            });\n",
       "          </script>\n",
       "          "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: __main__.py [-h] --train-files TRAIN_FILES [TRAIN_FILES ...]\n",
      "                   [--num-epochs NUM_EPOCHS]\n",
      "                   [--train-batch-size TRAIN_BATCH_SIZE]\n",
      "                   [--eval-batch-size EVAL_BATCH_SIZE]\n",
      "                   [--eval-files EVAL_FILES [EVAL_FILES ...]]\n",
      "                   [--embedding-size EMBEDDING_SIZE]\n",
      "                   [--first-layer-size FIRST_LAYER_SIZE]\n",
      "                   [--num-layers NUM_LAYERS] [--scale-factor SCALE_FACTOR]\n",
      "                   --job-dir JOB_DIR\n",
      "                   [--verbosity {DEBUG,ERROR,FATAL,INFO,WARN}]\n",
      "                   [--train-steps TRAIN_STEPS] [--eval-steps EVAL_STEPS]\n",
      "                   [--export-format {JSON,CSV,EXAMPLE}]\n",
      "__main__.py: error: argument --train-files is required\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "\t# ---------------------------------------------\n",
    "\t# command parsing from Google ML Engine Example\n",
    "\t# ---------------------------------------------\n",
    "\tparser = argparse.ArgumentParser()\n",
    "\t# Input Arguments\n",
    "\tparser.add_argument(\n",
    "\t  '--train-files',\n",
    "\t  help='GCS or local paths to training data',\n",
    "\t  nargs='+',\n",
    "\t  required=True\n",
    "\t)\n",
    "\tparser.add_argument(\n",
    "\t  '--num-epochs',\n",
    "\t  help=\"\"\"\\\n",
    "\t  Maximum number of training data epochs on which to train.\n",
    "\t  If both --max-steps and --num-epochs are specified,\n",
    "\t  the training job will run for --max-steps or --num-epochs,\n",
    "\t  whichever occurs first. If unspecified will run for --max-steps.\\\n",
    "\t  \"\"\",\n",
    "\t  type=int,\n",
    "\t)\n",
    "\tparser.add_argument(\n",
    "\t  '--train-batch-size',\n",
    "\t  help='Batch size for training steps',\n",
    "\t  type=int,\n",
    "\t  default=40\n",
    "\t)\n",
    "\tparser.add_argument(\n",
    "\t  '--eval-batch-size',\n",
    "\t  help='Batch size for evaluation steps',\n",
    "\t  type=int,\n",
    "\t  default=40\n",
    "\t)\n",
    "\n",
    "\t# -------------------------------\n",
    "\t# If evaluation file is prepared,\n",
    "\t# change 'required' value\n",
    "\t# -------------------------------\n",
    "\tparser.add_argument(\n",
    "\t  '--eval-files',\n",
    "\t  help='GCS or local paths to evaluation data',\n",
    "\t  nargs='+',\n",
    "\t  required=False\n",
    "\t)\n",
    "\t# Training arguments\n",
    "\tparser.add_argument(\n",
    "\t  '--embedding-size',\n",
    "\t  help='Number of embedding dimensions for categorical columns',\n",
    "\t  default=8,\n",
    "\t  type=int\n",
    "\t)\n",
    "\tparser.add_argument(\n",
    "\t  '--first-layer-size',\n",
    "\t  help='Number of nodes in the first layer of the DNN',\n",
    "\t  default=100,\n",
    "\t  type=int\n",
    "\t)\n",
    "\tparser.add_argument(\n",
    "\t  '--num-layers',\n",
    "\t  help='Number of layers in the DNN',\n",
    "\t  default=4,\n",
    "\t  type=int\n",
    "\t)\n",
    "\tparser.add_argument(\n",
    "\t  '--scale-factor',\n",
    "\t  help='How quickly should the size of the layers in the DNN decay',\n",
    "\t  default=0.7,\n",
    "\t  type=float\n",
    "\t)\n",
    "\tparser.add_argument(\n",
    "\t  '--job-dir',\n",
    "\t  help='GCS location to write checkpoints and export models',\n",
    "\t  required=True\n",
    "\t)\n",
    "\n",
    "\t# Argument to turn on all logging\n",
    "\tparser.add_argument(\n",
    "\t  '--verbosity',\n",
    "\t  choices=[\n",
    "\t      'DEBUG',\n",
    "\t      'ERROR',\n",
    "\t      'FATAL',\n",
    "\t      'INFO',\n",
    "\t      'WARN'\n",
    "\t  ],\n",
    "\t  default='INFO',\n",
    "\t)\n",
    "\t# Experiment arguments\n",
    "\tparser.add_argument(\n",
    "\t  '--train-steps',\n",
    "\t  help=\"\"\"\\\n",
    "\t  Steps to run the training job for. If --num-epochs is not specified,\n",
    "\t  this must be. Otherwise the training job will run indefinitely.\\\n",
    "\t  \"\"\",\n",
    "\t  type=int\n",
    "\t)\n",
    "\tparser.add_argument(\n",
    "\t  '--eval-steps',\n",
    "\t  help='Number of steps to run evalution for at each checkpoint',\n",
    "\t  default=100,\n",
    "\t  type=int\n",
    "\t)\n",
    "\tparser.add_argument(\n",
    "\t  '--export-format',\n",
    "\t  help='The input format of the exported SavedModel binary',\n",
    "\t  choices=['JSON', 'CSV', 'EXAMPLE'],\n",
    "\t  default='JSON'\n",
    "\t)\n",
    "\n",
    "\targs = parser.parse_args()\n",
    "\n",
    "\t# Set python level verbosity\n",
    "\ttf.logging.set_verbosity(args.verbosity)\n",
    "\t# Set C++ Graph Execution level verbosity\n",
    "\tos.environ['TF_CPP_MIN_LOG_LEVEL'] = str(\n",
    "\t  tf.logging.__dict__[args.verbosity] / 10)\n",
    "\n",
    "\t# Run the training job\n",
    "\thparams=hparam.HParams(**args.__dict__)\n",
    "\trun_experiment(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
